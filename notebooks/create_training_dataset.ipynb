{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/qwj/code/HDInstruct\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/home/qwj/hfcache\"\n",
    "os.environ['DATASETS_OFFLINE'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Six datasets are used in this notebook\n",
    "\n",
    "- CoQA Train Split\n",
    "- Squad v2.0 Train Split\n",
    "- TriviaQA Train Split\n",
    "- NQ Train Split\n",
    "- LAMA Trex\n",
    "- InternalStates TrueOrFalse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_URL = \"https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json\"\n",
    "assert os.path.exists(\"./data/raw/coqa-train-v1.0.json\"), f\"Download from {_URL} and save to ./data/raw/\"\n",
    "\n",
    "def generate_coqa_examples(filepath = \"./data/raw/coqa-train-v1.0.json\"):\n",
    "    data = json.load(open(filepath))\n",
    "    for story in data['data']:\n",
    "        story_id = story['id']\n",
    "        context = story['story']\n",
    "        for i, (q, a) in enumerate(zip(story['questions'], story['answers'])):\n",
    "            # train split does not have additional answers\n",
    "            # additional_answers = [story['additional_answers'][str(j)][i]['input_text'] for j in range(3)]\n",
    "            answers = list(set([a['input_text']]))\n",
    "            yield  {\n",
    "                \"id\": f\"coqa_{story_id}_{i}\",\n",
    "                \"context\": context,\n",
    "                \"question\": q['input_text'],\n",
    "                \"ground_truth\": answers,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108647,\n",
       " {'id': 'coqa_3zotghdk5ibi9cex97fepx7jetpso7_0',\n",
       "  'context': 'The Vatican Apostolic Library (), more commonly called the Vatican Library or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much older, it is one of the oldest libraries in the world and contains one of the most significant collections of historical texts. It has 75,000 codices from throughout history, as well as 1.1 million printed books, which include some 8,500 incunabula. \\n\\nThe Vatican Library is a research library for history, law, philosophy, science and theology. The Vatican Library is open to anyone who can document their qualifications and research needs. Photocopies for private study of pages from books published between 1801 and 1990 can be requested in person or by mail. \\n\\nIn March 2014, the Vatican Library began an initial four-year project of digitising its collection of manuscripts, to be made available online. \\n\\nThe Vatican Secret Archives were separated from the library at the beginning of the 17th century; they contain another 150,000 items. \\n\\nScholars have traditionally divided the history of the library into five periods, Pre-Lateran, Lateran, Avignon, Pre-Vatican and Vatican. \\n\\nThe Pre-Lateran period, comprising the initial days of the library, dated from the earliest days of the Church. Only a handful of volumes survive from this period, though some are very significant.',\n",
       "  'question': 'When was the Vat formally opened?',\n",
       "  'ground_truth': ['It was formally established in 1475']})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data = list(generate_coqa_examples())\n",
    "len(processed_data), processed_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_config = {\n",
    "    \"raw_path\": \"./data/raw/coqa-train-v1.0.json\",\n",
    "    \"num_examples\": len(processed_data),\n",
    "    \"columns\": [\"id\", \"context\", \"question\", \"ground_truth\"],\n",
    "    \"data\": processed_data,\n",
    "}\n",
    "\n",
    "# save to file\n",
    "with open(\"./data/processed_train/coqa_train.json\", \"w\") as f:\n",
    "    json.dump(data_with_config, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQUAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "_URL = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\"\n",
    "assert os.path.exists(\"./data/raw/squad-train-v2.0.json\"), f\"Download from {_URL} and save to ./data/raw/\"\n",
    "\n",
    "def generate_squad_examples(filepath = \"./data/raw/squad-train-v2.0.json\"):\n",
    "    data = json.load(open(filepath))\n",
    "    for article in data['data']:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                if qa['is_impossible']:\n",
    "                    continue\n",
    "                question = qa['question']\n",
    "                answers = set([a['text'] for a in qa['answers']])\n",
    "                yield {\n",
    "                    \"id\": f\"squad_{qa['id']}\",\n",
    "                    \"context\": context,\n",
    "                    \"question\": question,\n",
    "                    \"ground_truth\": list(answers),\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 86821\n",
      "{'id': 'squad_56be85543aeaaa14008c9063', 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".', 'question': 'When did Beyonce start becoming popular?', 'ground_truth': ['in the late 1990s']}\n"
     ]
    }
   ],
   "source": [
    "processed_data = list(generate_squad_examples())\n",
    "print(f\"Number of examples: {len(processed_data)}\")\n",
    "print(processed_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_config = {\n",
    "    \"raw_path\": \"./data/raw/squad-train-v2.0.json\",\n",
    "    \"num_examples\": len(processed_data),\n",
    "    \"columns\": [\"id\", \"context\", \"question\", \"ground_truth\"],\n",
    "    \"data\": processed_data,\n",
    "}\n",
    "\n",
    "with open(\"./data/processed_train/squad_train.json\", \"w\") as f:\n",
    "    json.dump(data_with_config, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TriviaQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since trivia_qa couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'rc.nocontext' at /home/qwj/hfcache/datasets/trivia_qa/rc.nocontext/0.0.0/0f7faf33a3908546c6fd5b73a660e0f8ff173c2f (last modified on Wed Mar  6 12:47:32 2024).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'question_id', 'question_source', 'entity_pages', 'search_results', 'answer'],\n",
       "    num_rows: 138384\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triviaqa_ds = datasets.load_dataset(\"trivia_qa\", \"rc.nocontext\", split='train', cache_dir=\"/home/qwj/hfcache/datasets\")\n",
    "triviaqa_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check duplications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def generate_triviaqa_examples(triviaqa_ds):\n",
    "    qid_record = defaultdict(list)\n",
    "    for i, example in enumerate(triviaqa_ds):\n",
    "        qid_record[example['question_id']].append(i)\n",
    "    for qid, indices in qid_record.items():\n",
    "        # assert questions are the same\n",
    "        all_questions, all_answers = set(), set()\n",
    "        for i in indices:\n",
    "            all_questions.add(triviaqa_ds[i]['question'])\n",
    "            all_answers.add(triviaqa_ds[i]['answer']['value'])\n",
    "            for alias in triviaqa_ds[i]['answer']['aliases']:\n",
    "                all_answers.add(alias)\n",
    "        if len(all_questions) != 1:\n",
    "            print(f\"question {qid}, {indices} has multiple questions:\\n\",'\\n'.join(all_questions))\n",
    "            continue\n",
    "        yield {\n",
    "            \"id\": f\"triviaqa_{qid}\",\n",
    "            \"question\": list(all_questions),\n",
    "            \"ground_truth\": list(all_answers),\n",
    "        }\n",
    "\n",
    "shit = list(generate_triviaqa_examples(triviaqa_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so it does not have conflicting question ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \n",
    "data_with_config = {\n",
    "    \"raw_path\": \"datasets.load_dataset(\\\"trivia_qa\\\", \\\"rc.nocontex\\\", split=\\\"train\\\")\",\n",
    "    \"num_examples\": len(shit),\n",
    "    \"columns\": [\"id\", \"question\", \"ground_truth\"],\n",
    "    \"data\": shit,\n",
    "}\n",
    "\n",
    "with open(\"./data/processed_train/triviaqa_train.json\", \"w\") as f:\n",
    "    json.dump(data_with_config, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since nq_open couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'nq_open' at /home/qwj/hfcache/datasets/nq_open/nq_open/0.0.0/3e24b5c209e8f578bd6f5ee795167a3577674383 (last modified on Fri Mar  1 09:15:18 2024).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer'],\n",
       "    num_rows: 87925\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nq_ds = datasets.load_dataset(\"nq_open\", split='train', cache_dir=\"/home/qwj/hfcache/datasets\")\n",
    "nq_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nq_examples(nq_ds):\n",
    "    for i, example in enumerate(nq_ds):\n",
    "        yield {\n",
    "            \"id\": f\"nq_{i}\",\n",
    "            \"question\": example['question'],\n",
    "            \"ground_truth\": example['answer'],\n",
    "        }\n",
    "\n",
    "processed_data = list(generate_nq_examples(nq_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \n",
    "data_with_config = {\n",
    "    \"raw_path\": \"datasets.load_dataset(\\\"nq_open\\\", split=\\\"train\\\")\",\n",
    "    \"num_examples\": len(processed_data),\n",
    "    \"columns\": [\"id\", \"question\", \"ground_truth\"],\n",
    "    \"data\": processed_data,\n",
    "}\n",
    "\n",
    "with open(\"./data/processed_train/nq_train.json\", \"w\") as f:\n",
    "    json.dump(data_with_config, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAMA Trex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/qwj/hfcache/modules/datasets_modules/datasets/lama/430016dd70224564ad385a96e0e4a3f88aeb5beaf4e34a8cf65b390fbc83aed7 (last modified on Mon Mar 11 08:27:49 2024) since it couldn't be found locally at lama, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['uuid', 'obj_uri', 'obj_label', 'sub_uri', 'sub_label', 'predicate_id', 'sub_surface', 'obj_surface', 'masked_sentence', 'template', 'template_negated', 'label', 'description', 'type'],\n",
       "        num_rows: 1304391\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lama_ds = datasets.load_dataset('lama', 'trex')\n",
    "lama_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obj_label</th>\n",
       "      <th>sub_label</th>\n",
       "      <th>predicate_id</th>\n",
       "      <th>template</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Northamptonshire</td>\n",
       "      <td>A605 road</td>\n",
       "      <td>P131</td>\n",
       "      <td>[X] is located in [Y] .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Northamptonshire</td>\n",
       "      <td>A605 road</td>\n",
       "      <td>P131</td>\n",
       "      <td>[X] is located in [Y] .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Northamptonshire</td>\n",
       "      <td>A605 road</td>\n",
       "      <td>P131</td>\n",
       "      <td>[X] is located in [Y] .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Northamptonshire</td>\n",
       "      <td>A605 road</td>\n",
       "      <td>P131</td>\n",
       "      <td>[X] is located in [Y] .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Northamptonshire</td>\n",
       "      <td>A605 road</td>\n",
       "      <td>P131</td>\n",
       "      <td>[X] is located in [Y] .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          obj_label  sub_label predicate_id                 template\n",
       "0  Northamptonshire  A605 road         P131  [X] is located in [Y] .\n",
       "1  Northamptonshire  A605 road         P131  [X] is located in [Y] .\n",
       "2  Northamptonshire  A605 road         P131  [X] is located in [Y] .\n",
       "3  Northamptonshire  A605 road         P131  [X] is located in [Y] .\n",
       "4  Northamptonshire  A605 road         P131  [X] is located in [Y] ."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = lama_ds['train'].select_columns(['obj_label', 'sub_label', 'predicate_id', 'template']).to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34017"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50%正确，50%拿同一谓词的其他内容代替"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predict_ids = df['predicate_id'].unique()\n",
    "all_obj_labels = {\n",
    "    key: list(set(df[df['predicate_id'] == key]['obj_label']))\n",
    "    for key in all_predict_ids\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A605 road is located in Northamptonshire .</td>\n",
       "      <td>1</td>\n",
       "      <td>P131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kupreanof Island is located in Alaska .</td>\n",
       "      <td>1</td>\n",
       "      <td>P131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pershing County is located in Pasadena .</td>\n",
       "      <td>0</td>\n",
       "      <td>P131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Porcupine Hills is located in Manitoba .</td>\n",
       "      <td>1</td>\n",
       "      <td>P131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Minnesota State Highway 36 is located in Minne...</td>\n",
       "      <td>1</td>\n",
       "      <td>P131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           statement  label category\n",
       "0         A605 road is located in Northamptonshire .      1     P131\n",
       "1            Kupreanof Island is located in Alaska .      1     P131\n",
       "2           Pershing County is located in Pasadena .      0     P131\n",
       "3           Porcupine Hills is located in Manitoba .      1     P131\n",
       "4  Minnesota State Highway 36 is located in Minne...      1     P131"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statements = []\n",
    "labels = []\n",
    "categories = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    random_number = random.randint(0, 1)\n",
    "    if random_number == 0:\n",
    "        label = 0\n",
    "        while True:\n",
    "            obj = random.choice(all_obj_labels[row['predicate_id']])\n",
    "            if obj != row['obj_label']:\n",
    "                break\n",
    "    else:\n",
    "        obj = row['obj_label']\n",
    "        label = 1\n",
    "    statement = row['template'].replace('[X]', row['sub_label']).replace('[Y]', obj)\n",
    "    statements.append(statement)\n",
    "    labels.append(label)\n",
    "    categories.append(row['predicate_id'])\n",
    "\n",
    "new_df = pd.DataFrame({'statement': statements, 'label': labels, 'category': categories})\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save = {\n",
    "    \"raw_path\": \"datasets.load_dataset('lama', 'trex')\",\n",
    "    \"num_examples\": len(new_df),\n",
    "    \"columns\": [\"statement\", \"label\", \"category\"],\n",
    "    \"data\": new_df.to_dict(orient='records'),\n",
    "}\n",
    "\n",
    "with open(\"./data/processed_train/lama_trex_train.json\", \"w\") as f:\n",
    "    json.dump(data_to_save, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InternalStates TrueOrFalse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\n",
    "    \"animals_true_false.csv\",\n",
    "    \"companies_true_false.csv\",\n",
    "    \"facts_true_false.csv\",\n",
    "    \"inventions_true_false.csv\",\n",
    "    \"cities_true_false.csv\",\n",
    "    \"elements_true_false.csv\",\n",
    "    \"generated_true_false.csv\"\n",
    "]\n",
    "\n",
    "_URL = \"azariaa.com/Content/Datasets/true-false-dataset.zip\"\n",
    "for file_name in file_names:\n",
    "    assert os.path.exists(f\"./data/raw/True_or_False/{file_name}\"), f\"Download from {_URL} and save to ./data/raw/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6330"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_true_false_examples():\n",
    "    for file_name in file_names:\n",
    "        df = pd.read_csv(f\"./data/raw/True_or_False/{file_name}\")\n",
    "        for index, row in df.iterrows():\n",
    "            yield {\n",
    "                \"category\": file_name,\n",
    "                \"statement\": row['statement'],\n",
    "                \"label\": row['label'],\n",
    "            }\n",
    "\n",
    "len(list(generate_true_false_examples()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_true_false = list(generate_true_false_examples())\n",
    "data_to_save = {\n",
    "    \"raw_path\": \"https://azariaa.com/Content/Datasets/true-false-dataset.zip\",\n",
    "    \"num_examples\": len(generated_true_false),\n",
    "    \"columns\": [\"category\", \"statement\", \"label\"],\n",
    "    \"data\": generated_true_false,\n",
    "}\n",
    "\n",
    "with open(\"./data/processed_train/internal_states_train.json\", \"w\") as f:\n",
    "    json.dump(data_to_save, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
