{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个笔记本看，怎么计算suffix 新增的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json  merges.txt  pytorch_model.bin  tokenizer.json  vocab.json\n"
     ]
    }
   ],
   "source": [
    "!ls /data/MODELS/gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回顾Peft Prefix Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): GPT2LMHeadModel(\n",
      "    (transformer): GPT2Model(\n",
      "      (wte): Embedding(50257, 768)\n",
      "      (wpe): Embedding(1024, 768)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "      (h): ModuleList(\n",
      "        (0-11): 12 x GPT2Block(\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2Attention(\n",
      "            (c_attn): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "  )\n",
      "  (prompt_encoder): ModuleDict(\n",
      "    (default): PrefixEncoder(\n",
      "      (embedding): Embedding(20, 18432)\n",
      "    )\n",
      "  )\n",
      "  (word_embeddings): Embedding(50257, 768)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from peft import PrefixTuningConfig, get_peft_model, TaskType\n",
    "from peft.peft_model import PeftModelForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "peft_config = PrefixTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    num_virtual_tokens=20\n",
    ")\n",
    "model = get_peft_model(\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"/data/MODELS/gpt2\"),\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "(prompt_encoder): ModuleDict(\n",
    "    (default): PrefixEncoder(\n",
    "        (embedding): Embedding(20, 18432)\n",
    "    )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个模块是在`PeftModel._setup_prompt_encoder`里面加的\n",
    "\n",
    "Line 392\n",
    "```python\n",
    "if config.peft_type == PeftType.PROMPT_TUNING:\n",
    "    prompt_encoder = PromptEmbedding(config, self.word_embeddings)\n",
    "elif config.peft_type == PeftType.MULTITASK_PROMPT_TUNING:\n",
    "    prompt_encoder = MultitaskPromptEmbedding(config, self.word_embeddings)\n",
    "elif config.peft_type == PeftType.P_TUNING:\n",
    "    prompt_encoder = PromptEncoder(config)\n",
    "elif config.peft_type == PeftType.PREFIX_TUNING:\n",
    "    prompt_encoder = PrefixEncoder(config)\n",
    "else:\n",
    "    raise ValueError(\"Not supported\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Prefix Encoder For PrefixTuning\n",
    "\n",
    "```python\n",
    "def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.prefix_projection = config.prefix_projection\n",
    "    token_dim = config.token_dim\n",
    "    num_layers = config.num_layers\n",
    "    encoder_hidden_size = config.encoder_hidden_size\n",
    "    num_virtual_tokens = config.num_virtual_tokens\n",
    "    if self.prefix_projection and not config.inference_mode:\n",
    "        # Use a two-layer MLP to encode the prefix\n",
    "        self.embedding = torch.nn.Embedding(num_virtual_tokens, token_dim)\n",
    "        self.transform = torch.nn.Sequential(\n",
    "            torch.nn.Linear(token_dim, encoder_hidden_size),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(encoder_hidden_size, num_layers * 2 * token_dim),\n",
    "        )\n",
    "    else:\n",
    "        self.embedding = torch.nn.Embedding(num_virtual_tokens, num_layers * 2 * token_dim)\n",
    "```\n",
    "默认如果没有prefix_projection的话，就是只有Embedding模式；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以我们只需要 重载原有的forward就行了！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qwj/miniconda3/envs/mistral/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at /data/MODELS/gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2ForSequenceClassification(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"/data/MODELS/gpt2\")\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
